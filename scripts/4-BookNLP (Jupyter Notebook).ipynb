{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BookNLP\n",
    "\n",
    "Book NLP is a natural language processing pipeline for large texts. It performs the following tasks: \n",
    " * Part-of-speech tagging\n",
    " * Dependency parsing\n",
    " * Entity recognition\n",
    " * Character name clustering (e.g., \"Tom\", \"Tom Sawyer\", \"Mr. Sawyer\", \"Thomas Sawyer\" -> TOM_SAWYER) and coreference resolution\n",
    " * Quotation speaker identification\n",
    " * Supersense tagging (e.g., \"animal\", \"artifact\", \"body\", \"cognition\", etc.)\n",
    " * Event tagging\n",
    " * Referential gender inference (TOM_SAWYER -> he/him/his)\n",
    "\n",
    "This tutorial runs BookNLP on one and multiple plain text files. BookNLP results (5 different files generated per text) are saved in a folder on your local machine. Some simple analyses (counts, named entity extraction) are also performed. \n",
    "\n",
    "Learn more about BookNLP:\n",
    "* https://github.com/booknlp/booknlp\n",
    "* https://colab.research.google.com/drive/1c9nlqGRbJ-FUP2QJe49h21hB4kUXdU_k?usp=sharing#scrollTo=CVb7N2RDkLVm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-dependencies booknlp\n",
    "from booknlp.booknlp import BookNLP\n",
    "#!pip install spacy\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Model and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters of BookNLP model\n",
    "model_params={\n",
    "    \"pipeline\":\"entity,quote,supersense,event,coref\", \n",
    "    \"model\":\"big\", \n",
    "}\n",
    "\n",
    "booknlp=BookNLP(\"en\", model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/PATHNAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Book NLP on Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define folder where files are stored to run BookNLP on\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "\n",
    "#Loop to run book NLP on each file\n",
    "for f in files: \n",
    "    #Define filename as each file in folder\n",
    "    inputFile = f\n",
    "    #Set folder for booknlp results\n",
    "    outputDir = \"BookNLP_Results/\"\n",
    "    #Set ID as name of each file\n",
    "    idd = f\n",
    "    #Run book nlp\n",
    "    booknlp.process(inputFile, outputDir, idd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze BookNLP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TSV Files to CSV Files\n",
    "Convert entities, tokens, and supersense files to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change working directory to folder with BookNLP results\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Define new folder for csv files\n",
    "csvs = \"/PATHNAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob \n",
    "#Set file endings to look for\n",
    "tokens = 'tokens'\n",
    "supersense = ''\n",
    "endings = ('.entities', 'supersense', 'tokens')\n",
    "\n",
    "#Iterate over original BookNLP files\n",
    "for filename in os.listdir(path):\n",
    "    #Read tsv into a pandas dataframe\n",
    "    if filename.endswith(endings):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\", quoting=3)\n",
    "        #Write the dataframe to a csv file\n",
    "        df.to_csv(os.path.join(csvs, filename + '.csv'), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zip files in separate folders (for sharing purposes)\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with zipfile.ZipFile('booknlp_entities.zip', 'w') as newzip:\n",
    "    for filepath in glob.glob(\"*.txt.entities.csv\"):\n",
    "        newzip.write(filepath)\n",
    "\n",
    "with zipfile.ZipFile('booknlp_tokens.zip', 'w') as newzip:\n",
    "    for filepath in glob.glob(\"*.txt.tokens.csv\"):\n",
    "        newzip.write(filepath)\n",
    "\n",
    "with zipfile.ZipFile('booknlp_supersense.zip', 'w') as newzip:\n",
    "    for filepath in glob.glob(\"*.txt.supersense.csv\"):\n",
    "        newzip.write(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore BookNLP Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Open NER file for one book\n",
    "for filepath in glob.glob(\"FILENAME.csv\"):\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    ner_df = pd.read_csv(filepath)\n",
    "# Print the resulting dataframe\n",
    "ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open token file for one book \n",
    "for filepath in glob.glob(\"FILENAME.csv\"):\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    pos_df = pd.read_csv(filepath)\n",
    "# Print the resulting dataframe\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Open supersense file for one book \n",
    "for filepath in glob.glob(\"FILENAME.csv\"):\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    supersense_df = pd.read_csv(filepath)\n",
    "# Print the resulting dataframe\n",
    "supersense_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find All Groups of Named Entities\n",
    "\n",
    "Several groups of named entities may be of interest to researchers: people, organizations, locations, and vehicles.\n",
    "\n",
    "BookNLP tags the following person-based named entities: \n",
    "* PERSON: People, including fictional.\n",
    "\n",
    "BookNLP tags the following person-based named entities: \n",
    "* ORG: Companies, agencies, institutions, etc.\n",
    "\n",
    "BookNLP tags the following place and space-based named entities: \n",
    "* FAC: Buildings, airports, highways, bridges, etc\n",
    "* GPE: Countries, cities, states\n",
    "* LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "\n",
    "BookNLP tags the following vehicle named entities: \n",
    "* VEH: Vehicles\n",
    "\n",
    "This code finds all instances of all groups and appends them to new dataframe column for each text.\n",
    "\n",
    "More about named entity tags: https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import to check progress of code\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Get location named entities in each file and append to a dataframe\n",
    "filename_list = []\n",
    "loc_lists = []\n",
    "\n",
    "for filepath in glob.glob(\"*.txt.entities.csv\"):\n",
    "    #Add filename to list of file names\n",
    "    filename_list.append(filepath)\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    df = pd.read_csv(filepath)\n",
    "    #Define list for locations in df\n",
    "    loc_list = []\n",
    "    #Iterate through dataframe and check if NE category is LOC\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if row['cat'] == 'LOC':\n",
    "            loc_list.append(row['text'])\n",
    "        if row['cat'] == 'GPE':\n",
    "            loc_list.append(row['text'])\n",
    "        if row['cat'] == 'FAC':\n",
    "            loc_list.append(row['text'])\n",
    "    loc_lists.append(loc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine words into one string\n",
    "import itertools\n",
    "\n",
    "#Combine list of lists into one big list\n",
    "master_loc_list = list(itertools.chain.from_iterable(loc_lists))\n",
    "master_loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all words are strings\n",
    "string_loc_list = [str(word) for word in master_loc_list]\n",
    "\n",
    "#Join words in list to string\n",
    "text = ' '.join(string_loc_list)\n",
    "\n",
    "#Remove punctuation\n",
    "text = text.translate(str.maketrans('', ''. string.punctuation))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words, width=800, height=800, background_color='white', max_words=50)\n",
    "\n",
    "wordcloud.generate(text)\n",
    "\n",
    "#Plot word cloud\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 20  most frequent locations in corpus\n",
    "from collections import Counter\n",
    "import string \n",
    "\n",
    "#Tokenize text\n",
    "tokens = [token.lower() for token in nltk.word_tokenize(text) if token.lower() not in stop_words]\n",
    "\n",
    "#Count word frequencies\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "#Get top 20 most common words\n",
    "top_words = dict(word_freq.most_common(20))\n",
    "\n",
    "#Plot word frequencies\n",
    "plt.bar(top_words.keys(), top_words.values())\n",
    "plt.title('Top 20 Most Frequent Words')\n",
    "plt.xlabel('Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import to check progress of code\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Find the people, orgs and vehicles\n",
    "filename_list = []\n",
    "people_lists = []\n",
    "org_lists = []\n",
    "vehicle_lists = []\n",
    "\n",
    "for filepath in glob.glob(\"*.txt.entities.csv\"):\n",
    "    #Add filename to list of file names\n",
    "    filename_list.append(filepath)\n",
    "    # Read the .entities.txt file into a pandas dataframe\n",
    "    df = pd.read_csv(filepath)\n",
    "    #Define list for locations in df\n",
    "    people_list = []\n",
    "    org_list = []\n",
    "    veh_list = []\n",
    "    #Iterate through dataframe and check if NE category is LOC\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if row['cat'] == 'PERSON':\n",
    "            # If it is, append data fram the text column to the list\n",
    "            people_list.append(row['text'])\n",
    "        if row['cat'] == 'ORG':\n",
    "            # If it is, append data fram the text column to the list\n",
    "            org_list.append(row['text'])\n",
    "        if row['cat'] == 'VEH':\n",
    "            # If it is, append data fram the text column to the list\n",
    "            veh_list.append(row['text'])\n",
    "    #Append location list to big location list\n",
    "    people_lists.append(people_list)\n",
    "    org_lists.append(org_list)\n",
    "    veh_lists.append(veh_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df with filenames and location entities\n",
    "locations_df = pd.DataFrame({'Book': filename_list, 'Locations': loc_lists, 'People': people_lists})\n",
    "locations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define function to count number of each type of named entity in each file\n",
    "def extract_entity_counts(file):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    doc = nlp(text)\n",
    "    entity_counts = {}\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ not in entity_counts:\n",
    "            entity_counts[entity.label_] = 1\n",
    "        else: \n",
    "            entity_counts[entity.label_] += 1\n",
    "    return entity_counts\n",
    "\n",
    "#Create empty df to store results\n",
    "columns = ['file','entity_type', 'count']\n",
    "df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "#Loop over all BookNLP .book files in the directory\n",
    "for filepath in glob.glob(\"*.book\"):\n",
    "    #Extract named entity from the file\n",
    "    entity_counts = extract_entity_counts(filepath)\n",
    "    print(entity_counts)\n",
    "\n",
    "#Print resulting dataframe\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
