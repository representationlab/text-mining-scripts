{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sOwRC3vIdVf"
   },
   "source": [
    "# Segment and Disaggregate Texts\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Use this code to clean, section, and disaggregate texts and corpora. \n",
    "\n",
    "**Why Perform Text Sectioning?** \n",
    "\n",
    "Dividing texts into sections (for example, chapters or chunks of N length) is valuable as a precursor to topic modeling and other forms of computational analysis which perform more accurately when applied to groups of segmented documents from longer texts. \n",
    "\n",
    "**Why Disaggregate Texts?** \n",
    "\n",
    "The process of disaggregating the words in texts (in this case, by alphabetizing them) also creates data sets that can be shared freely where original texts cannot be due to copyright restrictions. \n",
    "\n",
    "*Input/Output Specifications:* \n",
    "\n",
    "This code requires plain txt files as input, either those from this repository's sample_data folder or those from a local machine. It returns csv files with disaggregated text grouped by chapter or chunk of n length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFlH8OZYDYS2"
   },
   "source": [
    "## Upload and Add Text Files To Pandas DataFrame\n",
    "In this section, text files are added into a Pandas DataFrame. Pandas is a fast and relatively easy way to work with large datasets. Though data frames are typically associated with numbers, Pandas also offers many functionalities for [working with textual data. ](https://www.tutorialspoint.com/python_pandas/python_pandas_working_with_text_data.htm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import os and glob\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Import nltk for tokenization \n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/PATHNAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append all txt files to a pandas dataframe\n",
    "filenames = []\n",
    "data = []\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "for f in files:\n",
    "    #if f.endswith('.txt'):\n",
    "        with open(f, 'rb') as myfile:\n",
    "            filenames.append(myfile.name)\n",
    "            data.append(myfile.read())\n",
    "d = {'Title':filenames, 'Text': data}\n",
    "books = pd.DataFrame(d)\n",
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-74TxYNkiGbi"
   },
   "source": [
    "## Perform Minimal Cleaning and Set Parameters for Sectioning \n",
    "Several basic cleaning processes are implemented: removing unwanted characters from titles and removing encoding  and newline characters from texts. Parameters are then set for part(s) of text to be included in sectioning. In the SciFi Corpus project, \"START OF BOOK\" and \"END OF BOOK\" tags were added to delineate the body of each text. Code in this section removes any text outside the starting and ending parameters--e.g., title page, copyright page, other paratext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_cleaned = books.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove .txt from titles\n",
    "books_cleaned['Title'] = books_cleaned['Title'].str.replace(r'.txt', ' ', regex=True) \n",
    "books_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
    "books_cleaned['Text'] = books_cleaned['Text'].apply(lambda x: x.decode('utf-8', errors=\"ignore\"))\n",
    "\n",
    "#Remove newline characters\n",
    "books_cleaned['Text'] = books_cleaned['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
    "books_cleaned['Text'] = books_cleaned['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
    "books_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ob2j0r6YZsOP"
   },
   "outputs": [],
   "source": [
    "#Check that text is cleaned and sectioned\n",
    "books_cleaned.iloc[0]['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS7KWmxq3HQo"
   },
   "source": [
    "## Section Texts By Chunks of N Length\n",
    "When working with texts WITHOUT discernable chapter headings--or, even if chapter headings are present but too infrequent to split texts into meaningful segments--texts can instead be sectioned by chunks of \"N\" length, where N is a variable that can be custom-set below. After checking the word counts for each text to determine what size chunks would be appropriate, this code iterates through the texts and splits them each time it counts \"N\" number of words. From here, the text from each chunk is appended to a new dataframe and denoted by book and chunk number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWE_NqN-CO4s"
   },
   "outputs": [],
   "source": [
    "#Get number of words in each book (helps to determine chunk length)\n",
    "words = books_cleaned[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "#Append chapter counts to dataframe\n",
    "books_cleaned[\"Word Count\"] = words\n",
    "books_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxuTbkEZNKhz"
   },
   "outputs": [],
   "source": [
    "#Tokenize Text\n",
    "books_cleaned['Text'] = books_cleaned['Text'].astype(str)\n",
    "books_cleaned['Tokens'] = books_cleaned.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
    "books_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GLee_W4ZC4G"
   },
   "outputs": [],
   "source": [
    "#Define chunking function\n",
    "def split(list_a, chunk_size):\n",
    "  for i in range(0, len(list_a), chunk_size):\n",
    "    yield list_a[i:i + chunk_size]\n",
    "\n",
    "#Set desired size of chunks\n",
    "chunk_size = 500\n",
    "\n",
    "#Create new list for chunked sentences\n",
    "chunked_sentences = []\n",
    "\n",
    "#Perform chunking function on each row of tokens\n",
    "s = books_cleaned['Tokens']\n",
    "for content in s:\n",
    "  chunks = list(split(content, chunk_size))\n",
    "  #Check that text is being chunked correctly\n",
    "  print(chunks[0])\n",
    "  #Add to new list\n",
    "  chunked_sentences.append(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcgktYQzb34m"
   },
   "outputs": [],
   "source": [
    "#Create dictionary to associate chunks with titles\n",
    "keys = books_cleaned['Title']\n",
    "values = chunked_sentences\n",
    "\n",
    "res = {keys[i]: values[i] for i in range(len(keys))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ruZQhgc3AP"
   },
   "outputs": [],
   "source": [
    "#Add chunks to new dataframe\n",
    "chunked_df = pd.DataFrame.from_dict(res, orient='index')\n",
    "chunked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdjZTi9adr2q"
   },
   "outputs": [],
   "source": [
    "#Reset dataframe index and rename columns\n",
    "chunked_df = chunked_df.stack().reset_index()\n",
    "chunked_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
    "chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSnpX8s8enM8"
   },
   "outputs": [],
   "source": [
    "#Tidying the DF\n",
    "#Combine book and chunk labels into one column\n",
    "chunked_df['Book + Chunk'] = chunked_df['Title'].astype(str) + ' Chunk ' + chunked_df['Chunk'].astype(str)\n",
    "\n",
    "#Remove individual book and chunk columns\n",
    "chunked_df.drop(columns=['Title', 'Chunk'])\n",
    "\n",
    "#Detokenize text\n",
    "TreebankWordDetokenizer().detokenize\n",
    "chunked_df['Text'] = chunked_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
    "chunked_df['Text'] \n",
    "\n",
    "#Reindex so book + chunk is first column \n",
    "column_names = \"Book + Chunk\", \"Text\"\n",
    "chunked_df = chunked_df.reindex(columns=column_names)\n",
    "\n",
    "#Print cleaned df\n",
    "chunked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CSV and Txt Output of Aggregated and Disaggregated Texts \n",
    "\n",
    "At this point, you have three dataframes containing segmented texts that are ready for further analysis. All three (along with the dataframe containing the full texts) can be downloaded as csv files. Depending on the nature of your texts and future analysis, it may be necessary to first disaggregate the data before download. Some analyses like topic modeling work well with \"bag of words\" data, and copyrighted texts cannot be shared in their original forms. Disaggregation, or the breakdown of data into smaller (disordered) parts, is accomplished through the alphabetization of the words in each chapter/chunk.Below, texts are disaggregated and the resulting dataframes can then be downloaded as csvs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change working directory to where output will be stored\n",
    "path = os.chdir(\"PATHNAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download CSVs of aggregated texts\n",
    "\n",
    "#Download CSV with aggregated full texts \n",
    "books_agg = books_cleaned[['Title', 'Text']]\n",
    "books_agg.to_csv('full_texts_agg_output.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "#Download CSV with aggregated chunks\n",
    "chunked_df.to_csv('chunks_agg_output.csv', encoding = 'utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disaggregate data in each dataframe\n",
    "\n",
    "#Alphabetize words in each full text\n",
    "books_bow = books_agg.copy()\n",
    "books_bow['Text'] = books_bow['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
    "\n",
    "#Alphabetize words in each chunk \n",
    "chunked_df['Text'] = chunked_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download CSVs of disaggregated texts\n",
    "\n",
    "#Download CSV with disaggregated full texts \n",
    "books_bow.to_csv('full_texts_bow_output.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "#Download disaggregated chunks to csv\n",
    "chunked_df.to_csv('chunks_bow_output.csv', encoding = 'utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the disaggregated TXT files\n",
    "output_directory = 'output_folder'\n",
    "\n",
    "# Iterate through DataFrame rows and save each text as a TXT file\n",
    "for index, row in books_bow.iterrows():\n",
    "    text = row['Text']\n",
    "    file_name = f'file_{index + 1}.txt'\n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(f\"File '{file_name}' saved.\")\n",
    "\n",
    "print(\"All files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the disaggregated chunked TXT files\n",
    "output_directory = 'output_folder'\n",
    "\n",
    "# Iterate through DataFrame rows and save each text as a TXT file\n",
    "for index, row in chunked_df.iterrows():\n",
    "    text = row['Text']\n",
    "    file_name = f'file_{index + 1}.txt'\n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(f\"File '{file_name}' saved.\")\n",
    "\n",
    "print(\"All files saved.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPsG8HSErXqZR20E1VCVLwZ",
   "include_colab_link": true,
   "name": "Text Sectioning and Disaggregation in Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
