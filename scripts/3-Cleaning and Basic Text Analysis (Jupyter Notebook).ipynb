{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Basic Text Analysis\n",
    "\n",
    "Methods for cleaning the segmented and disaggregated text files and performing word counts, chapter counts, stopword removal, and other basic frequency calculations and enrichment processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages and Upload Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "#Get dictionary of English words to keep \n",
    "from nltk.corpus import words\n",
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory to location of segmented texts\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Upload dataframe with segmented texts\n",
    "clean_df = pd.read_csv('FILENAME.csv')\n",
    "\n",
    "#Drop first column (unnamed)\n",
    "clean_df = clean_df.iloc[: , 1:]\n",
    "\n",
    "#Make text column string values\n",
    "clean_df['Text'] = clean_df['Text'].astype(str)\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase all words\n",
    "clean_df['Clean_Text'] = clean_df['Text'].str.lower()\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(r'[^\\w\\s]+')\n",
    "clean_df['Clean_Text'] = [p.sub(' ', x) for x in clean_df['Clean_Text'].tolist()]\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extraneous whitespace using regular expressions\n",
    "clean_df['Clean_Text'] = clean_df['Clean_Text'] .str.replace('  +', ' ', regex=True)\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers and extraneous characters\n",
    "clean_df['Clean_Text'] = clean_df['Clean_Text'] .str.replace('\\d+', '', regex=True)\n",
    "clean_df['Clean_Text'] = clean_df['Clean_Text'] .str.replace('_', '')\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change path to where you want to save the files\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Save cleaned dataframe to working directory\n",
    "clean_df.to_csv('FILENAME.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Cleaning: Stopword Removal, Lemmatization and Keep Only English Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new dataframe for advanced cleaning\n",
    "adv_clean_df = clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "adv_clean_df['Text_NoStops'] = adv_clean_df['Clean_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "adv_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define list of words to keep from nltk words\n",
    "#Set function will make processing faster\n",
    "words_list = words.words()\n",
    "my_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words corpus does not contain plural forms, must lemmatize first\n",
    "#nltk.download('omw-1.4')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#Can choose to lemmatize clean text with or without stopwords\n",
    "adv_clean_df['Text_Lemmas'] = adv_clean_df['Clean_Text'].apply(lambda x: ' '.join([wnl.lemmatize(word) for word in x.split() ]))\n",
    "adv_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all text to a list of strings\n",
    "adv_clean_df['Text_Lemmas'] = adv_clean_df['Text_Lemmas'].astype(str)\n",
    "data = adv_clean_df.Text_Lemmas.values.tolist()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append each word in list of strings to list of words\n",
    "all_words = []\n",
    "\n",
    "for text in data:\n",
    "    word = text.split()\n",
    "    all_words.append(word)\n",
    "    \n",
    "import itertools\n",
    "all_words_list = list(itertools.chain(*all_words))\n",
    "len(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only words in lemma list that are also in words corpus\n",
    "adv_clean_df['English_Text'] = adv_clean_df['Text_Lemmas'].apply(lambda x: ' '.join([word for word in x.split() if word in (my_words)]))\n",
    "adv_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all English text to a list of strings\n",
    "adv_clean_df['English_Text'] = adv_clean_df['English_Text'].astype(str)\n",
    "kept_data = adv_clean_df.English_Text.values.tolist()\n",
    "kept_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append each word in list of strings to list of words\n",
    "kept_words = []\n",
    "\n",
    "for text in kept_data:\n",
    "    word = text.split()\n",
    "    kept_words.append(word)\n",
    "    \n",
    "import itertools\n",
    "kept_words_list = list(itertools.chain(*kept_words))\n",
    "len(kept_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of words that have been removed from the text\n",
    "removed_list = set(all_words_list) - set(kept_words_list)\n",
    "len(removed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine what words were removed from the text\n",
    "removed_list= list(removed_list)\n",
    "removed_list.sort()\n",
    "removed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the list of removed words into a dataframe\n",
    "col_name = ['Removed Words']\n",
    "removed_words_df = pd.DataFrame(removed_list, columns = col_name)\n",
    "removed_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean stopwords from list of English text and add to new column\n",
    "adv_clean_df['English_Text_NoStops'] = adv_clean_df['English_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "adv_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change path to where you want to save the files\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Save dataframe with kept words and titles\n",
    "adv_clean_df.to_csv('FILENAME.csv', index=False)\n",
    "\n",
    "#Saved removed words dataframe to working directory\n",
    "removed_words_df.to_csv('Removed_Words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get new dataframe to work with\n",
    "df_counts = adv_clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of words in each chunk\n",
    "#Make sure to use original texts (not cleaned)\n",
    "ch_words = df_counts[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "#Append word counts of each chapter chunk to dataframe\n",
    "df_counts[\"Word Count\"] = ch_words\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get most frequent words across the dataframe\n",
    "#Use text in English/without stopwords\n",
    "Counter(\" \".join(df_counts[\"Text_NoStops\"]).split()).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concordancing\n",
    "\n",
    "# Convert the 'text' column to a list of tokens for each row\n",
    "df_counts['tokens'] = df_counts['text'].apply(word_tokenize)\n",
    "\n",
    "# Create an NLTK Text object for concordancing\n",
    "text_object = Text(word for tokens in df['tokens'] for word in tokens)\n",
    "\n",
    "# Define the target word\n",
    "target_word = \"sentence\"\n",
    "\n",
    "# Specify the number of words before and after the target word for concordancing\n",
    "context_size = 5\n",
    "\n",
    "# Perform concordancing\n",
    "concordance_list = text_object.concordance_list(target_word, width=context_size * 2)\n",
    "\n",
    "# Display the concordance lines\n",
    "for line in concordance_list:\n",
    "    print(line.line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import spacy\n",
    "!pip install spaCy\n",
    "\n",
    "# Install English language model\n",
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import spacy\n",
    "import spacy\n",
    "\n",
    "# Load spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Import os to upload documents and metadata\n",
    "import os\n",
    "\n",
    "# Import pandas DataFrame packages\n",
    "import pandas as pd\n",
    "\n",
    "# Import graphing package\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nlp pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Check what functions it performs\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df for analysis\n",
    "#Get new dataframe to work with\n",
    "df_spaCy = clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that runs the nlp pipeline on any given input text\n",
    "def process_text(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the \"Text\" column, so that the nlp pipeline is called on each student essay\n",
    "df_spaCy['Doc'] = df_spaCy['Text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve tokens from a doc object\n",
    "def get_token(doc):\n",
    "    return [(token.text) for token in doc]\n",
    "\n",
    "# Run the token retrieval function on the doc objects in the dataframe\n",
    "df_spaCy['Tokens'] = df_spaCy['Doc'].apply(get_token)\n",
    "df_spaCy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_lemma(doc):\n",
    "    return [(token.lemma_) for token in doc]\n",
    "\n",
    "# Run the lemma retrieval function on the doc objects in the dataframe\n",
    "df_spaCy['Lemmas'] = df_spaCy['Doc'].apply(get_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_pos(doc):\n",
    "    #Return the coarse- and fine-grained part of speech text for each token in the doc\n",
    "    return [(token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "# Define a function to retrieve parts of speech from a doc object\n",
    "df_spaCy['POS'] = df_spaCy['Doc'].apply(get_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract proper nouns from Doc object\n",
    "def extract_proper_nouns(doc):\n",
    "    return [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "# Apply function to Doc column and store resulting proper nouns in new column\n",
    "df_spaCy['Proper_Nouns'] = df_spaCy['Doc'].apply(extract_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract named entities from doc objects\n",
    "def extract_named_entities(doc):\n",
    "    return [ent.label_ for ent in doc.ents]\n",
    "\n",
    "# Apply function to Doc column and store resulting named entities in new column\n",
    "df_spaCy['Named_Entities'] = df_spaCy['Doc'].apply(extract_named_entities)\n",
    "df_spaCy['Named_Entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract text tagged with named entities from doc objects\n",
    "def extract_named_entities(doc):\n",
    "    return [ent for ent in doc.ents]\n",
    "\n",
    "# Apply function to Doc column and store resulting text in new column\n",
    "df_spaCy['NE_Words'] = df_spaCy['Doc'].apply(extract_named_entities)\n",
    "df_spaCy['NE_Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first Doc object\n",
    "doc = df_spaCy['Doc'][1]\n",
    "\n",
    "# Visualize named entity tagging in a single paper\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
