{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic Modeling\n",
    "BERTopic is a topic modeling tool which creates topic clusters based on word embeddings and a class-based TF-IDF. It generates a set of topics, the top words in each topic, and the likelihood of each text in a corpus belonging to each topic. Visualizations can also be generated based on the relationships between topics.  \n",
    "\n",
    "\n",
    "This notebook uses BERTopic for unsupervised topic modeling in order to explore the sci-fi corpus. BERTopic can be customized to support the following types of topic modeling:\n",
    "* Guided: seeded topics manually set by the researcher\n",
    "* (Semi)-supervised: modeling guided by document labels\n",
    "* Hierarchicial: topic similarity and rankings calculated, subtopics generated\n",
    "* Dynamic: differentiates topic clustering based on doc timestamps\n",
    "* Online: modeling updated incrementally from small batches of texts \n",
    "\n",
    "Adapted from:\n",
    "\n",
    "https://github.com/MaartenGr/BERTopic/blob/master/notebooks/BERTopic.ipynb\n",
    "\n",
    "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing#scrollTo=y_eHBI1jSb6i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install bertopic\n",
    "#!pip install --upgrade bertopic\n",
    "#!conda install pandas\n",
    "#!conda install nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "#Get dictionary of English words to keep \n",
    "from nltk.corpus import words\n",
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "#Import BERTopic\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get current working directory \n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#Change working directory\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Upload dataframe√\n",
    "df = pd.read_csv('FILENAME.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data type to string\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "\n",
    "#Append data to list\n",
    "text = df.English_Text.to_list()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run BERTopic Model\n",
    "\n",
    "The model (`topic_model`) can be defined based on multiple parameters, including: \n",
    "* language: language of word embedding model used (default=English)\n",
    "* embedding-model: sentence-transformers model which is used to create word embeddings; defaults to pre-set model, and [here's a list of all available models](https://www.sbert.net/docs/pretrained_models.html)\n",
    "* nr_topics: set to reduce number of topics; can specify a  specific # of topics OR set as \"auto\" to merge topics with similarity > 0.9\n",
    "* calculate_probabilities: calculates likelihood of each document falling into any of the possible documents (set to True or False)\n",
    "* vectorizer_model: Removes stopwords after embeddings are created\n",
    "* verbose: set to True so model initiation process does not shows messages\n",
    "a\n",
    "Once the model is defined, fit it to the corpus prepared above using `fit_transform` and get topics and probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set environment variable to false to avoid error\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Set vectorizer model to remove stopwords after embeddings have been created\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Create new topic model\n",
    "topic_model = BERTopic(language=\"english\", nr_topics = 'auto', vectorizer_model=vectorizer_model, calculate_probabilities=True, verbose=True, top_n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run topic model on texts\n",
    "topics, probs = topic_model.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the 10 most frequent topics (-1 topic refers to all outliers, ignore it)\n",
    "freq = topic_model.get_topic_info(); freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate top n words in a specific topic\n",
    "topic_model.get_topic(3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predicted topics for the first 10 documents in corpus\n",
    "topic_model.topics_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a dataframe which has info about top topic in each document\n",
    "topics_df = topic_model.get_document_info(text)\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add document names to dataframe\n",
    "\n",
    "#Create dataframe with texts and titles\n",
    "texts_df = df[['Book + Chunk','Text']].copy()\n",
    "texts_df.rename(columns={'Book + Chunk':'Title','Text':'Document'}, inplace=True)\n",
    "texts_df\n",
    "\n",
    "#Merge with above dataset on Document\n",
    "top_BERTopic_per_doc = pd.DataFrame()\n",
    "top_BERTopic_per_doc = texts_df.merge(topics_df, how='right',on='Document')\n",
    "top_BERTopic_per_doc\n",
    "\n",
    "#Sort by topic (or title)\n",
    "top_BERTopic_per_doc.sort_values(by=['Topic'], inplace=True)\n",
    "top_BERTopic_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change path to where you want to save the files\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Download CSV with document and topic information\n",
    "top_BERTopic_per_doc.to_csv('FILENAME.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make CSV with just topic number and top words \n",
    "BERTopic_topic_metadata_df = top_BERTopic_per_doc[['Topic','Name','Top_n_words']].copy()\n",
    "BERTopic_topic_metadata_df = BERTopic_topic_metadata_df.drop_duplicates()\n",
    "BERTopic_topic_metadata_df = BERTopic_topic_metadata_df.reset_index(drop=True)\n",
    "BERTopic_topic_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download as CSV\n",
    "BERTopic_topic_metadata_df.to_csv('FILENAME.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add topic names (determined manually) to a new dataframe\n",
    "topic_names = pd.read_csv('FILENAME.csv')\n",
    "topic_names = topic_names.drop(columns='Top_n_words')\n",
    "topic_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add topic name to dataframe \n",
    "named_topics_per_doc = top_BERTopic_per_doc.copy()\n",
    "\n",
    "named_topics_per_doc['Topic'] = top_BERTopic_per_doc.Topic.map(topic_names.set_index('Topic')['Name'])\n",
    "named_topics_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort texts by topic\n",
    "named_topics_per_doc.sort_values(by=['Topic'], inplace=True)\n",
    "named_topics_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download CSV with named topics\n",
    "named_topics_per_doc.to_csv('FILENAME.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Span Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove document, probability and representative document first (will mess up duplicates) and name (redundant)\n",
    "counted_topics = named_topics_per_doc[['Title','Topic']].copy()\n",
    "\n",
    "#Sort texts by title\n",
    "counted_topics.sort_values(by=['Title'], inplace=True)\n",
    "counted_topics\n",
    "\n",
    "counted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of times each topic appears in each text\n",
    "from collections import Counter\n",
    "\n",
    "df1 = counted_topics['Topic'].apply(lambda x: pd.Series(Counter(x.split(','))), 1).fillna(0).astype(int)\n",
    "\n",
    "counted_topics = counted_topics.join(df1.add_suffix(' Count'))\n",
    "counted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download couned topics df to csv\n",
    "counted_topics.to_csv('FILENAME.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new dataframe to track topic prevalence over the years\n",
    "yearly_topics = counted_topics.copy()\n",
    "\n",
    "#Split title on year heading and remove following text\n",
    "test = yearly_topics['Title'].str.split(\"_\", expand = True)\n",
    "\n",
    "yearly_topics['Title'] = test[0]\n",
    "yearly_topics.rename(columns={\"Title\": \"Year\"}, inplace=True)\n",
    "yearly_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_topics = yearly_topics.groupby(['Year']).sum()\n",
    "yearly_topics = yearly_topics.reset_index()\n",
    "yearly_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select topics of interest\n",
    "interest_topics = yearly_topics[['Year', 'INSERT TOPICS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import seaborn for graphing and melt dataframe to prepare for plot (will shift topic counts into column)\n",
    "import seaborn as sns\n",
    "\n",
    "dfm = interest_topics.melt('Year', var_name='Topics', value_name='vals')\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot usage of topics over tile\n",
    "ax = sns.pointplot(x=\"Year\", y=\"vals\", hue='Topics', data=dfm)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.title(\"Topic Usage Over Time\")\n",
    "plt.rcParams[\"figure.figsize\"] = (25,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds Per Topic of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import word cloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Remove custom words (stopwords and names not previously filtered out)\n",
    "custom_stop_words = ['INSERT']\n",
    "\n",
    "def create_wordcloud(model, topic):\n",
    "    text = {word: value for word, value in model.get_topic(topic) if word not in custom_stop_words}\n",
    "    w = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    w.generate_from_frequencies(text)\n",
    "    plt.imshow(w, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show wordcloud\n",
    "print('Topic: INSERT')\n",
    "create_wordcloud(topic_model, topic=61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Topic: INSERT')\n",
    "create_wordcloud(topic_model, topic=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Topic: INSERT')\n",
    "create_wordcloud(topic_model, topic=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Topic: INSERT')\n",
    "create_wordcloud(topic_model, topic=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Topic: INSERT')\n",
    "create_wordcloud(topic_model, topic=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve All Books Containing a Specific Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe\n",
    "topic_contents = named_topics_per_doc[['Title','Topic']].copy()\n",
    "\n",
    "topic_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Keep only books which have specific topic\n",
    "topic_contents = topic_contents.loc[topic_contents['Topic'] == 'INSERT']\n",
    "\n",
    "#Sort by title\n",
    "topic_contents = topic_contents.sort_values(by='Title',ascending=True)\n",
    "\n",
    "#Drop duplicates\n",
    "topic_contents = topic_contents.drop_duplicates(subset=[\"Title\"], keep='first')\n",
    "topic_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Topic Usage Between Two Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new df for topics per author analysis\n",
    "topics_per_author = counted_topics.copy()\n",
    "topics_per_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split book on first hyphen, keep only text after first hyphen (author name, title, and chunk)\n",
    "start = topics_per_author[\"Title\"].str.split(\"_\", expand = True)\n",
    "topics_per_author['Title'] = start[1]\n",
    "\n",
    "#Split book on second hyphen keep text only before second hyphen (author name)\n",
    "end = topics_per_author[\"Title\"].str.split(\"_\", expand = True)\n",
    "topics_per_author['Title'] = end[0]\n",
    "\n",
    "topics_per_author.rename(columns={\"Title\": \"Author\"}, inplace=True)\n",
    "topics_per_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of times each author uses each topic\n",
    "from collections import Counter\n",
    "\n",
    "df1 = topics_per_author['Topic'].apply(lambda x: pd.Series(Counter(x.split(','))), 1).fillna(0).astype(int)\n",
    "\n",
    "topics_per_author = topics_per_author.join(df1.add_suffix(' Count'))\n",
    "topics_per_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_author = topics_per_author.groupby(['Author']).sum()\n",
    "topics_per_author = topics_per_author.reset_index()\n",
    "topics_per_author.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose authors of interest\n",
    "select_authors = topics_per_author[topics_per_author.Author.str.contains('ALDISS|LEGUIN')] \n",
    "select_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dfm2 = select_authors.melt('Author', var_name='Topics', value_name='vals')\n",
    "\n",
    "#Only keep rows where topic value is not 0\n",
    "dfm2 = dfm2[dfm2.vals != 0]\n",
    "\n",
    "#Remove outlier rows and other extraneous rows\n",
    "dfm2 = dfm2[dfm2[\"Topics\"].str.contains(\"Outliers\")==False]\n",
    "dfm2 = dfm2[dfm2[\"Topics\"].str.contains(\"Book_Program\")==False]\n",
    "dfm2 = dfm2[dfm2[\"Topics\"].str.contains(\"UNCLEAR\")==False]\n",
    "\n",
    "\n",
    "dfm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bar plots based on topic counts per author\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "for i, Author in enumerate(dfm2.Author.unique()):\n",
    "    _df = dfm2[dfm2.Author==Author].sort_values(by='Topics')\n",
    "    g = sns.barplot(\n",
    "        ax=ax[i],\n",
    "        data=_df,\n",
    "        x='Author', y='vals',\n",
    "        hue='Topics'\n",
    "    )\n",
    "    ax[i].set(\n",
    "        title=f'Topic Usage By ' + Author\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Most Representative Documents Per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copy of dataframe\n",
    "most_rep = named_topics_per_doc[['Title','Topic','Representative_document']].copy()\n",
    "\n",
    "#change data type to string\n",
    "most_rep['Representative_document'] = most_rep['Representative_document'].astype(str)\n",
    "\n",
    "most_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only most representative documents\n",
    "most_rep = most_rep.loc[most_rep['Representative_document'] == 'True']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort by title\n",
    "most_rep = most_rep.sort_values(by='Title',ascending=True)\n",
    "\n",
    "#Drop duplicates\n",
    "most_rep = most_rep.drop_duplicates(subset=[\"Title\"], keep='first')\n",
    "\n",
    "most_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all documents representative of a certain topic\n",
    "most_rep.loc[most_rep['Topic'] == 'INSERT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize distance between topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probability that topics will appear in a specific document\n",
    "topic_model.visualize_distribution(probs[0], min_probability=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize hierarchical structure of topics\n",
    "topic_model.visualize_hierarchy(top_n_topics=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize top terms in selected topics\n",
    "topic_model.visualize_barchart(top_n_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create matrix to indicate similarity between topics\n",
    "topic_model.visualize_heatmap(n_clusters=20, width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the decline of c-TF-IDF score when adding words to the topic representation. \n",
    "#It allows you, using the elbow method, the select the best number of words in a topic.\n",
    "topic_model.visualize_term_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Topics for Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search for topics that are similar to an input search_term\n",
    "similar_topics, similarity = topic_model.find_topics(\"space\", top_n=5); similar_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at other terms in that one of the similar topics\n",
    "topic_model.get_topic(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the BERTopic Model\n",
    "\n",
    "Two common ways to update the topic model are based on ngram counts (default is single words, but you can also get bigrams, trigrams, etc) and by setting the number of topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update topics based on ngram counts\n",
    "topic_model.update_topics(text, n_gram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the topics again\n",
    "topic_model.get_topic_info(); freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(4)   # We select topic that we viewed before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce number of topics\n",
    "topic_model.reduce_topics(text, nr_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info(); freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare LDA and BERT Topics for Each Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change working directory\n",
    "path = os.chdir(\"/PATHNAME\")\n",
    "\n",
    "#Upload dataframe√\n",
    "lda_df = pd.read_csv('FILENAME.csv')\n",
    "\n",
    "#Rename columns to avoid confusion\n",
    "lda_df = lda_df.rename(columns={\"Dominant_Topic\": \"Dominant_LDA_Topic\", \"Topic_Perc_Contrib\": \"LDA_Topic_Perc_Contrib\", \"Keywords\": \"LDA_Topic_Keywords\"})\n",
    "lda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copy of bertopic df\n",
    "bertopic_df = named_topics_per_doc.copy()\n",
    "\n",
    "#Rename columns to avoid confusion\n",
    "bertopic_df = bertopic_df.rename(columns={\"Topic\": \"BERTopic_Topic\", \"Top_n_words\": \"BERTopic_Topic_Keywords\", \"Probability\": \"BERTopic_Probability\", \"Representative_document\": \"BERTopic_Representative_document\"})\n",
    "bertopic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dfs\n",
    "lda_and_bertopic_df = pd.merge(lda_df, bertopic_df, on=\"Title\")\n",
    "\n",
    "#Remove duplicates\n",
    "lda_and_bertopic_df = lda_and_bertopic_df.drop_duplicates()\n",
    "\n",
    "lda_and_bertopic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download combined dataframe\n",
    "lda_and_bertopic_df.to_csv('lda_and_bertopic_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sources\n",
    "Word Embeddings: https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/\n",
    "\n",
    "BERTopic Intro: https://towardsdatascience.com/meet-bertopic-berts-cousin-for-advanced-topic-modeling-ea5bf0b7faa3\n",
    "\n",
    "MOre about BERTopic: \n",
    "https://towardsdatascience.com/dynamic-topic-modeling-with-bertopic-e5857e29f872"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
